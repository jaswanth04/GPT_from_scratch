{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:  1115394\n"
     ]
    }
   ],
   "source": [
    "print('length of dataset: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping from characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Each character is converted to an integer, so there is a character level tokenizer. \n",
    "# we get long sequences\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data to train and val split\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), output is 47\n",
      "when input is tensor([18, 47]), output is 56\n",
      "when input is tensor([18, 47, 56]), output is 57\n",
      "when input is tensor([18, 47, 56, 57]), output is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), output is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), output is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), output is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), output is 58\n"
     ]
    }
   ],
   "source": [
    "# The transformers are auto regressive, hence the output, once again becomes an input and will be used for prediction\n",
    "\n",
    "# we consider block size to be 8\n",
    "\n",
    "block_size = 8\n",
    "\n",
    "x = train[:block_size]\n",
    "# Offsetting y by 1, so that always a set of characters predict the next set in an auto regressive fashion\n",
    "y = train[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    print(f'when input is {x[:t+1]}, output is {y[t]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape, xb: torch.Size([4, 8]), yb: torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'Shape, xb: {xb.shape}, yb: {yb.shape}')\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bigram Language Model below, we are creating a representation of each token that is generated above. \n",
    "\n",
    "The method that we use to do this, is create an embedding table, keep looking it up based on the index of the token. For example, token 1 will look up the 1st index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Changing the shape so that the cross entropy function can understand\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Implement the negative log likelihood\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Predictions for a given index\n",
    "            logits, _ = self(idx)\n",
    "\n",
    "            # We only use the last time step\n",
    "            logits = logits[:, -1, :] # dimension is B, C\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "\n",
    "            # append sample index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m(xb, yb)\n",
    "\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4316647052764893\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_steps = 25000\n",
    "\n",
    "for steps in range(num_steps):\n",
    "\n",
    "    # sample batch \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cl, IZEEDUENGonthe t l me's angice;\n",
      "Thewareratulyendutonde 's flie,\n",
      "fr arnd ic-mybem t m'dsent ee, blouged fere fearau se fofouriey'd!\n",
      "LANEThellld ogo withend tequ\n",
      "Y ldill, totooousothive wdd nctonasngin mm se mpll,\n",
      "WA st?\n",
      "\n",
      "Whins t'eng t rere t inchyig y;\n",
      "Y:\n",
      "S:\n",
      "RDYOn h cas re be I it? meswed,\n",
      "Noun\n",
      "Thou that alizathive ly\n",
      "\n",
      "Premo Twe HAus hyon,\n",
      "Hep s taldr g d wh h ju pr gurkeortwhin Whakil te d;\n",
      "Berenu ad-priou y!\n",
      "Buerstoo, engin, anifiterit p,\n",
      "isthey thopis'd\n",
      "Bu?\n",
      "He thilere apre thior ane; ofar pls swhengity, dyoloyororhes\n",
      "Cisthe is thirnon f bapsard bo whithatheam athik ousend d deeroleed! s win pehin, wouers, IORDORY:\n",
      "Orkitonce\n",
      "\n",
      "Ort ce\n",
      "O:\n",
      "ffit herom mem, f bourothalurerug wnrourer; a ho os thay a ke we be yod man te d lokngheheare te st: I,\n",
      "She norofrean is:\n",
      "Andore icard appilant ds the parexier ofl tor l.\n",
      "\n",
      "A ousamame s Bed:\n",
      "aurso we our su'st son,\n",
      "Tha IVERDe haru tlarof, in a I ft wn,\n",
      "He, bads be the.\n",
      "Am om w iofuruekerth,\n",
      "THad ING d:\n",
      "Munt, ce wan AGLAupsh! fe owe therethind, ay m: st er wien lyoror t haniley ns avend weputhoom valar anat o.\n",
      "ANGllesshowl\n",
      "Nuis,\n",
      "I des mageather omy, Gotourtogeend gshe ft akicetovean lon:\n",
      "'sturerean t, met thesll s stherstr lode nchinthers; agnd s h ntovef anaryom odias wores a be ose heang keange,\n",
      "Eines y marmyo wathe alde morome tous r w, aronte tr m; the tithetotsis sit te.\n",
      "The ayornonlor lo t suoplul meishouton d.\n",
      "MEd athifr INRY ibbjurshou fagras.\n",
      "ALomid ere mand win.\n",
      "Pralde, f myortwnveicot ace the pe t.\n",
      "An ld l f FFLel berel ser rthita ber If hot h m\n",
      "\n",
      "I gotshe,\n",
      "IShe watustis.\n",
      "Th I atst ratthio sir I ues cheretr r nf.\n",
      "Wigs mou wior! f medave,\n",
      "N toumpray t, er;\n",
      "\n",
      "\n",
      "G ilid m\n",
      "Whtseshe e I stizendo linchirehatoulf'ly ghathesthory prir.\n",
      "Calathin achiluthe we?\n",
      "\n",
      "\n",
      "\n",
      "sthe thith furda visccoweepepe on:\n",
      "CARI thond?\n",
      "che tat weroouredvimbry win alistormeree botomof ist'dre shisesep so,\n",
      "Ans ra I ourind,\n",
      "Hof th arm lceatho bt had way maraf l\n",
      "\n",
      "D:\n",
      "NThate'de, bs? vee Byouth\n",
      "D: H: gshit ateroveswencof id\n",
      "Theear,\n",
      "NT:\n",
      "\n",
      "Ang vemu\n",
      "THor ourd, he hanon s my ben the scot D f kngeadw whig ter ir VOHUKI t nt mybeeanas,\n",
      "My d s l s f leralt reaprang afot aleas; t nthe\n",
      "\n",
      "Wey tlyom: aronthin te sem, ck irowaithes rn, d,\n",
      "Whe, hs y t, t,\n",
      "AThegore l the fy te harem!\n",
      "Thilos R:\n",
      "Prom gr EER te btll,\n",
      "BRDWARE:\n",
      "Whicer? he des on y wa Laty pe, at hant D MINARBo ry tised yol, lyow wisiotird eeras illeepl Olang nenganelo end PSory tove; ty CENCA:\n",
      "Pa wond,\n",
      "Thard w my I risu hie anerere tr fr her ccld tirout nasusstheteno fare ctha weary il t.\n",
      "ARS:\n",
      "By: hin g nd\n",
      "\n",
      "Ise ds\n",
      "An d it p hy blldeantheerdind Toulpulisf feswe m de, t IZEThee\n",
      "Buritlplairf n!\n",
      "GLow, OKEd f s donsut. oues urat od oure HAUS: ithir r outhithe cand har Burowid, mamiowil ome.\n",
      "piterth\n",
      "TYOMof pantoutared y d mp y\n",
      "\n",
      "Ay ment ngem; matay,\n",
      "ENUSe mappayosure ir y p.\n",
      "ARith ange lld hend menen thoullold man t ie'sefidivenoun in t or terourmou hale ithat che o wira e!\n",
      "YButhandecad I acayorre g iglila mou but wipole so.\n",
      "Whrstthe y,\n",
      "Shist otoo y min, bon toue: chtes ine s st, iso. PHo seno OFRUCoprm make gare.\n",
      "YBith I f\n",
      "Tos y de mad he malag ity A:\n",
      "IUELENI mershoucon werd d tesoonoth?\n",
      "\n",
      "\n",
      "\n",
      "Wer:\n",
      "Myozericur msdo out th fa, my gnin mithim gn obundilisoug mor nofatheak t, Clseat o, d o 'TMALLAns\n",
      "WAUSULE wier gertout tha crisit wrired t t No mend, at, y s tod head, he:\n",
      "\n",
      "We y u owioiflader chathanenef mart frs ce me,-\n",
      "STIUSiter M:\n",
      "APr nt liconir ard LI whemathongen spre bethalare te gbas Wh ceny g yst s, inour-mepea y d ELALOPUCEYo y,\n",
      "Wand sinof ce:\n",
      "NTatouthirth ha\n",
      "Kelockn I:\n",
      "\n",
      "Asem.\n",
      "\n",
      "Fipo inou, ofand thoury we? t herdethy, owinceend tu t'lerir m t loun nt:\n",
      "Pangond f al meat He w narsther e t'd ove t fouth; in cout, alilo we meave werathe shacarouly way. bll thte usouth plis hersillditeawhes\n",
      "MABewno s mery: t fese m or stoueveive,\n",
      "SCHABUSAgo anaritho\n",
      "ORore\n",
      "TINOreve qu,\n",
      "LTons haund me o h, ayelapuricas\n",
      "\n",
      "ARLOrt hau g I y seo, bt wiorts te MERItid Gouend clan burousewsonanobere s grd t shis k supl IUK:\n",
      "Hoe bllerstwe s qun:\n",
      "Thire'd\n",
      "'d PSTh f sid agar to be army br VI gh id achipuly ston yourd'e stherepinean s goutame brerin howesun, Gotheeye, ME s\n",
      "AUDULIUShay ait, chin fast g'd Walfathe h me, fitheange f teees d mpend thiepelsor brd w wish, t Hemines hit hanese ho kithoond y t g t h kits is d at, men---d hel?\n",
      "bupa ghathessestrom isy led.\n",
      "CAn; wconyotrsenculou\n",
      "CHERod\n",
      "Andsiore huratet o; areatrfawigh?\n",
      "Whe ber bs.\n",
      "The.\n",
      "Bofuaraive ear my wan Bu ft mesthel ivevepiry As oy, har bespemag mou t tmeathang wit t te icur mice oundendoupou.\n",
      "Whenond t. y; IZof n:\n",
      "Ththine owhand mond 'sthid urran d or suds o t ameo t de,\n",
      "Go, ke; ky e th, y her nlst lontour, I sverod\n",
      "II'ss, y? s.\n",
      "IENI y helde alas me be so-be.\n",
      "Angesuroupoous!\n",
      "G I pllok d thad. y y, urine, hats by bree wawh ICKESe theru aseal sly in y, mibu featheat creass trs, beatheelarpieeth o'ceroby h pove ICENGRGrtheouss o the\n",
      "\n",
      "\n",
      "ARK: n am samesid din batstinof h'l's hinthathachomif re dece e thofto shain me wan gr llif hapes u.\n",
      "Butouthe\n",
      "Th\n",
      "\n",
      "Tomupotr, y,\n",
      "ANEY:\n",
      "ve ifl:\n",
      "HE:\n",
      "Tharery he, thingrmace:\n",
      "NGounthelind w pex'sardan,\n",
      "CH:\n",
      "\n",
      "desen,\n",
      "Theenthokised wird somurano abere ouchea squsure f or e's sif wig in arinkillu inouly IN the cong s,\n",
      "hadou I owr aycke th m thiarthol morinck t br,\n",
      "FLERKIOLINAnd stithar walvenacchad oun llltary?\n",
      "Fingunon stouinroresseosty. d\n",
      "Murio y I s atilllllltly ghaveer wibacome, t t nt?\n",
      "As matifur s g:\n",
      "\n",
      "TI I'd. te at, be' Rerd che dy; arode eat dersee m ks;\n",
      "HE:\n",
      "\n",
      "Wh picilizimince tisshome He.\n",
      "Angu qu wing?\n",
      "G e\n",
      "LO t h tou hin!\n",
      "RI I bilag e fowhe,\n",
      "\n",
      "\n",
      "Coure ELos aryoo we d thed cy lldeamathikeined thano choo nd can d d hath hie we, tl ndize, corst t tin:\n",
      "\n",
      "ERKI fe is rtlize d.\n",
      "AUCUKI noro RIt m steno mant we, isegheand wielllastolameono tich overeay wnganeshoer tode quizerem Unorswhitoforsorllont let we ntr burer,\n",
      "Whay.\n",
      "\n",
      "Cond dewe wharellknlsut,\n",
      "WI ay ker'\n",
      "Goon'dacrsaitalalde t husun ntam de, t hithor, rf any he bl,\n",
      "\n",
      "Thon nged.\n",
      "PHood\n",
      "To ngrinceed eas but!\n",
      "\n",
      "HI h codincourore t; one wideinou o w,-broubuge nery was myoumprmofritot, fl MBalfould us tss\n",
      "\n",
      "I bllyo, tut t? my dife\n",
      "SIs m ckntrime RY be tloure se' boy.\n",
      "S:\n",
      "HAn s; he taiou lir apou shisin t a am h me thusou tome sh wet alecudoou! th ame K:\n",
      "\n",
      "TED:\n",
      "Fowo be\n",
      "Whespl-d s, th he, m thand CEShe thireempis wimais toucive tt anorrroriou?\n",
      "ak, ou II we'rdithas IUThacat I'd blvevil the beaveanand shantr thet male s tronoose?\n",
      "O,\n",
      "APAUME:\n",
      "I bobindrsig theraff fantyom. both r ngnde, youg ETHintondeave emor I ty n.\n",
      "Secorge d tt ancem hearse s y\n",
      "Buld w whin as athathe,\n",
      "Budy kny.\n",
      "\n",
      "Thow!\n",
      "Done igulathotomeman amildom: y,\n",
      "IUTus heerembuntshorenst at m, t caiseess, gitlouesiseshance hefoatetank thow, g mo matelitone bu n bu?\n",
      "And s,\n",
      "y wit, as storr ath angs muclleave, carrde t.\n",
      "Whtt su ncrsthe ginshine ' t meveine h, kicowh atote b'sstake Grgan la isan isaty y.'dr pre t, Her nclas; m bousar w swik or,\n",
      "Y:\n",
      "\n",
      "Ifed I whery, de!\n",
      "I mldithattr w m, rrder far, s omilath farud hougal pe ashoupantoughoupre:\n",
      "\n",
      "MEYesve t y h, sponed rlislompobet d\n",
      "TURIfithy, t y nooine't I:\n",
      "\n",
      "LAnomy\n",
      "Frk cowacaneweer tof\n",
      "Sim is,\n",
      "angng m: hate surd kseend'l se d t;\n",
      "Whigomby d t ptre.\n",
      "HAno grerny corayon:\n",
      "\n",
      "Whes se Sithy mithecan w, her\n",
      "The\n",
      "HEEROLO:\n",
      "\n",
      "Ho wneloushapathiethexeanvent int anganes'din\n",
      "Tha odis fe, g w ILUSover, s ong u thyofar spre tingamee nompan my, il\n",
      "YO'Tovend thiormice, athealoue amewe soldes ars: t JUSTirorthe, hthausk profint.\n",
      "TAnsiserof od and onoou el ous\n",
      "And itonouremeiatlod ceathe mitlashime, ulars h, frse.\n",
      "JUCAn rdeit,\n",
      "\n",
      "st-we Se wimewisyo qutarpllanda d tr we RO:\n",
      "H: frrngoviran lfrthert whet pryorar,\n",
      "Parame peme, k\n",
      "HEreloue\n",
      "Aso'sea, atout meale Rag aimthille t ghthalencurexicrowors wouge K: us ' S: tse too-bildat d th ARYCKEDY:\n",
      "Do\n",
      "Palang LAn:\n",
      "LUKithathal! y p d ger forear aprackel y t se h haimemathe\n",
      "ANRDUS:\n",
      "LE:\n",
      "Muse binobyot m,\n",
      "Wiras OFeiure we, pth bur,\n",
      "RYes?\n",
      "CHak tee isthad ther ay then;\n",
      "Anto bo I't minggelin:\n",
      "LI theat bd tha s\n",
      "Hod f a nd Fr just y sheea d,\n",
      "\n",
      "\n",
      "\n",
      "WINTyo b,-g:\n",
      "OLARDead, lly, llourme le k Peyea me ide, hes ulld OLLay cles mbunoutomomur.\n",
      "Cade t,\n",
      "Auio t sunghy me.\n",
      "le.\n",
      "Ty I tll; bebe he moo er d suer y fomeey adise anikno whorel--I ffo mathigndst-pofarorsia po itts: Couk pas, wisenengrorth tepef his mbr CHAn ise minord here Y: st d s. m yoouncrikngert tfts tr gncoutyormare omay!\n",
      "Sed brmaroud ind tes sorit I llfel t,\n",
      "KEve:\n",
      "An,-haly tr V:\n",
      "Yowat aththts lase, wit:\n",
      "DYop ches mutin\n",
      "\n",
      "MAnd pl prathwe cthend thailie y falil y farmyoue g!\n",
      "\n",
      "INENGRY:\n",
      "MA gnd e\n",
      "PO:\n",
      "Ay:\n",
      "Thong nalu! ase cthimouttintourour n mershe! thithto Gomous an de.\n",
      "FFlo t m Hep,\n",
      "\n",
      "\n",
      "AMaronoory.\n",
      "Thtl, l Int t llirinnis.\n",
      "Y: h IZERCI uly.\n",
      "Tink g mivero y Yowind thellatrdyo we at s I irecof fe thyeendwat u prulovewoofldyouit.\n",
      "gsteldy:\n",
      "at teagelalend:\n",
      "ABuercowhe ure trsdoure rery!\n",
      "Tooo,\n",
      "\n",
      "Fouts one\n",
      "A t, hambyspain'sencke o terea t Leotoutlvoly, d topisoty ve bero, mod, ay shint us cavent, otl I forg ouse asebur bo, has pon; fas feseast r ofuicannged hat wamparotou akimy fondathor lat w JUSTEO:\n",
      "Sos ret at a ousth mbag s ar, ourcr Of r! t n:\n",
      "BAdis l t myom\n",
      "\n",
      "Wigs waist ususuer'l'l.\n",
      "ICALourccathand By, h ho Wearevetha war plee g.\n",
      "Find, pe\n",
      "Tofrwhrunt CLLe m at chr ll bame,\n",
      "Yorilitsthengon pis MBung imop\n",
      "I ORDot,\n",
      "Ashireyma ssuthesaly'se therngdwore as wer fuly, asoopha'llfe; thicout'ngiutord ame peorw, tr; h ell.\n",
      "OFol, cha NGo, ch I kncarouces the'dir,\n",
      "\n",
      "Ne ayof te, ils hangofo ito at n hey\n",
      "OFLI pe sp!\n",
      "\n",
      "S:\n",
      "Kind?\n",
      "A:\n",
      "Hat t GHEThou AHEd\n",
      "INThy sp.\n",
      "Fithend,\n",
      "Se, um me imest hin,\n",
      "Seshes weas, e mod wamyo KIONTUSor howen s r t s, t imonder be ft y arigh yous I my esouro whes I anse hik whemy, y fee.\n",
      "Thery\n",
      "t the pl ust couthardir.\n",
      "Twererllileitt, t d thand buin\n",
      "R:\n",
      "I aike ne bue Sp ne isst;\n",
      "Thakese k gigh acrestsatheve bllia intaved yeve, iso, ts, waty I E:\n",
      "MENofins g d tunciprse yoms; nelds s ar no t'celorsugh IImakn g;\n",
      "AHal alig hond torow wimy ak pe!\n",
      "Torigay hicureath t s ono thy'sthenipe g aveto ccone ilou s acrst INom d IINI wimatorol aveact?\n",
      "Ind f b'sery w co t.\n",
      "CY idies aneyofotoutres LE:\n",
      "AROFin, liceppof westof aierotonthir;\n",
      "O:\n",
      "ARor,\n",
      "JO:\n",
      "\n",
      "Tord owhit,\n",
      "ETovetonthen t io gl toot fatemaiceen tor:\n",
      "\n",
      "GEDWem:\n",
      "Any t ske\n",
      "CEd,\n",
      "LINE hes'd we; d.\n",
      "Oldd; rk sin tile no fouby\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical trick for self attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the tokens in a word or sentence to communicate with each other, also a token should not communicate with the future, as future needs to be predicted, it should only communicate with the past.\n",
    "\n",
    "A simple way of doing this, is taking the average of all the tokens preceeding the present token. This may not be efficient but, can be used as a starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Bag of words\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] \n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# The above approach is very inefficient, but can be done using triangular matrix multiplication\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "\n",
    "print(xbow2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above approach can also be made better and can provide some intuitive understanding for self attention\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "# Filling all the places where tril is zero to -inf\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# Taking softmax for weights\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 8\n",
    "\n",
    "torch.arange(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Single head to perform self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # B, T, head_size)\n",
    "\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "out = wei @ v # (B, T, head_size)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now wei has weights which will tell how influenced is the token when compared to the previous tokens\n",
    "\n",
    "# In encoder block, which is more about understanding, we would keep all the blocks. That means we will not be using tril\n",
    "# In decoder block, where we keep generating in the auto regressive fashion, we would not require the blocks to talk to the future blocks. Here we need to use trill\n",
    "\n",
    "wei[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self Attention - All keys, queries and values come from the same source\n",
    "\n",
    "Cross Attention - The keys, values come from decoder, and queries come from encoder\n",
    "\n",
    "Scaled Dot product attention - q, k has unit variance, but the wei has a variance of head_size. Hence to make it unit variance, we divide by sqrt(head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "384/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
